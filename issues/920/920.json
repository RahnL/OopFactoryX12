{
  "WorkItem": {
    "AffectedComponent": {
      "Name": "",
      "DisplayName": ""
    },
    "ClosedComment": "",
    "ClosedDate": "2012-07-27T01:12:43.29-07:00",
    "CommentCount": 0,
    "Custom": null,
    "Description": "Hello,\n \nI am using this for 835 files more than 150 KB. But it is not extracting any data. Kindly fix this issue.\n \nRam.",
    "LastUpdatedDate": "2013-05-16T04:34:03.41-07:00",
    "PlannedForRelease": "",
    "ReleaseVisibleToPublic": false,
    "Priority": {
      "Name": "Low",
      "Severity": 50,
      "Id": 1
    },
    "ProjectName": "x12parser",
    "ReportedDate": "2011-11-24T02:04:54.303-08:00",
    "Status": {
      "Name": "Closed",
      "Id": 4
    },
    "ReasonClosed": {
      "Name": "Unassigned"
    },
    "Summary": "Unable to Parse above 150 KB",
    "Type": {
      "Name": "Issue",
      "Id": 3
    },
    "VoteCount": 3,
    "Id": 920
  },
  "FileAttachments": [],
  "Comments": [
    {
      "Message": "I have seen the same problem, but with large 834 files. The current parser will rapidly consume all available memory and then produce bad/blank output. I have to process 834 files larger than 200MB, though the problem shows up on much smaller files. I do not want to use the unbundle tool, because this would result in almost 600,000 separate files for my large example.\r\n\r\nFor my purposes, I was able to make a small but ugly hack to the parser, to prevent building a full object tree and instead expose an event which is called for each direct child segment of the \"ST\" segments in the input file. I then wrote code to handle the event and  build up a custom output file.\r\n\r\nI would like to request that my scenario (of custom code to handle segments) be supported as part of any official fix for the input size limitation.\r\n\r\nAlso, I would like to express a slight preference for a 'pull' parser, in which the client code is in control of requesting the next piece of output from the parser. Such a parser would be more flexible when it comes to integrating with downstream processing code.\r\n",
      "PostedDate": "2011-11-25T15:35:08.18-08:00",
      "Id": -2147483648
    },
    {
      "Message": "I will look into adding an unbundle method that let's you specify the maximum size of the unbundled file so at least it can be batched into sizes that your machine can handle.\r\nI know that I've had problems running code on my netbook that go away when I run on a workstation.\r\nThe \"pull\" method might require some refactoring, so I might separate that out into anothe work item after I can add the extra unbundle method.",
      "PostedDate": "2011-12-06T06:39:48.637-08:00",
      "Id": -2147483648
    },
    {
      "Message": "",
      "PostedDate": "2012-05-05T11:33:57.68-07:00",
      "Id": -2147483648
    },
    {
      "Message": "Ran into the same situation as pjdennis when extracting 834's.  Got an \"out of memory exception.\"  Is there any chance of getting this resolved?  Was looking forward to using this tool but this could be a showstopper.",
      "PostedDate": "2012-06-15T20:26:07.387-07:00",
      "Id": -2147483648
    },
    {
      "Message": "",
      "PostedDate": "2012-06-15T20:27:03.293-07:00",
      "Id": -2147483648
    },
    {
      "Message": "Please let me know your system specifications, processor type, memory, operating system.\r\nIs it a laptop, desktop or server you are running on.",
      "PostedDate": "2012-06-16T10:41:06.837-07:00",
      "Id": -2147483648
    },
    {
      "Message": "My specs are a laptop running Win7 Pro SP1 64 bit, dual core i5 2.67ghz, 8 gigs of ram.  I watch the ram usage as it runs, and after about 10 seconds will spike but only to approx 4 gigs and then the out of memory exception comes back.",
      "PostedDate": "2012-06-16T11:43:10.723-07:00",
      "Id": -2147483648
    },
    {
      "Message": "A few other users have been able to get around this problem by using the UnbundleByLoopId which will also work at a transaction level to split their files before creating the XML.\r\nLoading the file into the Interchange object usually isn't the problem, it's running the serializer to create the XML.\r\nIf you are unable to unbundle, I can see if I can have a virtual way to unbundle before serializing.\r\nAlso, if you are just using your laptop for development/testing, perhaps you can manually truncate your files for testing and only use the full files on a production server.",
      "PostedDate": "2012-06-16T18:48:30.773-07:00",
      "Id": -2147483648
    },
    {
      "Message": "Associated with changeset 17851.",
      "PostedDate": "2012-07-09T07:39:29.77-07:00",
      "Id": -2147483648
    },
    {
      "Message": "Associated with changeset 17937.",
      "PostedDate": "2012-07-12T05:08:52.68-07:00",
      "Id": -2147483648
    },
    {
      "Message": "Resolved with changeset 18531.",
      "PostedDate": "2012-07-27T01:12:43.29-07:00",
      "Id": -2147483648
    },
    {
      "Message": "Sorry, for the delay in fixing this one.  \r\n\r\nI was able to reproduce this problem for file sizes as small as 50Mbytes.\r\nThe first fix I did was to change the Serialize method to write directly to a filestream instead of to memory first, this increased the file size you could read, but still has a limit that would cause the OutOfMemoryException.\r\n\r\nI also did add an X12StreamReader tht uses the \"pull\" method that you suggested.  This will break up the file into smaller batches attempting to add as many transactions as possible before it hits the MaxBatchSize in the app.config.  Of course since I am writing the files as I go, I do have to infer what the GE and IEA segment would look like since I have not yet reached those segments using a streaming parse.",
      "PostedDate": "2012-07-27T01:23:33.35-07:00",
      "Id": -2147483648
    },
    {
      "Message": "",
      "PostedDate": "2013-02-21T16:47:02.11-08:00",
      "Id": -2147483648
    },
    {
      "Message": "",
      "PostedDate": "2013-05-16T04:34:03.41-07:00",
      "Id": -2147483648
    }
  ]
}